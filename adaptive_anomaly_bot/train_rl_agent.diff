diff --git a/adaptive_anomaly_bot/train_rl_agent.py b/adaptive_anomaly_bot/train_rl_agent.py
index a9b1fd7..5793225 100644
--- a/adaptive_anomaly_bot/train_rl_agent.py
+++ b/adaptive_anomaly_bot/train_rl_agent.py
@@ -28,7 +28,7 @@ class BayesianAnomalyDetector:
 
     def compute_surprise(self, x):
         if self.mle_params is None and self.dist_type in ['beta', 'gamma']: return 0.5
-            
+
         if self.dist_type == 't':
             df, loc, scale = 2 * self.alpha_n, self.mu_n, np.sqrt(self.beta_n / (self.alpha_n * self.nu_n))
             if scale <= 0: return 0.5
@@ -41,7 +41,7 @@ class BayesianAnomalyDetector:
             a, loc, scale = self.mle_params
             if scale <= 0: return 0.5
             cdf_val, sf_val = gamma.cdf(x, a, loc=loc, scale=scale), gamma.sf(x, a, loc=loc, scale=scale)
-            
+
         return 1.0 - (2 * min(cdf_val, sf_val))
 
     def get_distribution_params(self):
@@ -75,61 +75,53 @@ class BayesianAnomalyDetector:
         try: self.mle_params = gamma.fit(data.clip(0.0001), floc=0)
         except Exception: pass
 
-def fetch_and_prepare_data(ticker="QQQ", period="2y", interval="1h"):
+def fetch_and_prepare_data(ticker="QQQ", period=None, interval="1h", start=None, end=None):
     print("--- Using robust merge_asof strategy with UTC conversion. ---")
-    
-    # --- STEP 1: Fetch HOURLY data for the main ticker ---
-    print(f"Fetching {interval} data for {ticker} over the last {period}...")
-    ticker_data = yf.Ticker(ticker).history(period=period, interval=interval)
-    if ticker_data.empty: 
+
+    print(f"Fetching {interval} data for {ticker}...")
+    ticker_data = yf.Ticker(ticker).history(period=period, interval=interval, start=start, end=end)
+    if ticker_data.empty:
         print(f"No data found for {ticker}")
         return None
 
-    # --- STEP 2: Fetch DAILY data for the VIX indicator ---
-    print(f"Fetching DAILY data for VIX over the last {period}...")
-    vix_data = yf.Ticker("^VIX").history(period=period, interval="1d")
+    print(f"Fetching DAILY data for VIX...")
+    vix_data = yf.Ticker("^VIX").history(period=period, interval="1d", start=start, end=end)
     if vix_data.empty:
         print("Warning: Could not fetch VIX data.")
         return None
-    
-    # --- STEP 3: The Definitive Timezone Fix ---
-    # Convert both indices to UTC to ensure they are compatible for merging.
+
     ticker_data.index = ticker_data.index.tz_convert('UTC')
     vix_data.index = vix_data.index.tz_convert('UTC')
-    
-    # Rename VIX close to 'vix' before merging
+
     vix_data.rename(columns={'Close': 'vix'}, inplace=True)
-    
-    # Use merge_asof to find the latest daily VIX for each hourly QQQ record
+
     data = pd.merge_asof(
         left=ticker_data.sort_index(),
         right=vix_data[['vix']].sort_index(),
         left_index=True,
         right_index=True,
-        direction='backward' # Use the most recent VIX value
+        direction='backward'
     )
-    
+
     print("Data fetched and aligned. Calculating indicators...")
     data['returns'] = (data['Close'] - data['Open']) / data['Open']
     data.ta.rsi(length=14, append=True)
     data.ta.atr(length=14, append=True)
-    
-    sma_long = data.ta.sma(close=data['Close'], length=200)
 
-    # --- THIS IS THE CORRECTED LINE ---
+    sma_long = data.ta.sma(close=data['Close'], length=200)
     data['sma_dist'] = (data['Close'] - sma_long) / sma_long
 
     data.rename(columns={"RSI_14": "rsi", "ATRr_14": "atr"}, inplace=True)
     data['rsi'] = data['rsi'] / 100.0
     atr_long = data.ta.sma(close=data['atr'], length=200)
     data['vol_regime'] = data['atr'] / atr_long
-    
+
     data.dropna(inplace=True)
-    
+
     if data.empty:
         print("CRITICAL ERROR: Dataframe is empty after indicator calculation and dropna().")
         return None
-        
+
     print(f"Data preparation complete. Final shape: {data.shape}")
     required_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'returns', 'rsi', 'atr', 'sma_dist', 'vol_regime', 'vix']
     return data[required_cols]
@@ -172,63 +164,50 @@ class TradingEnvironment(gym.Env):
         if self.position == 1 and current_price < self.stop_loss_price: action = 2
         elif self.position == -1 and current_price > self.stop_loss_price: action = 1
 
-        if action == 1: # Buy/Long
-            if self.position == -1: # Close short
+        if action == 1:
+            if self.position == -1:
                 self.balance = self.portfolio_value * (1 - self.trade_fee)
                 self.position, self.shares_held, self.entry_price = 0, 0, 0
                 trade_executed = True
-            elif self.position == 0: # Open long
+            elif self.position == 0:
                 self.position, self.entry_price = 1, current_price
                 self.stop_loss_price = current_price - (current_atr * self.stop_loss_atr_multiplier)
                 self.shares_held = self.balance / current_price * (1 - self.trade_fee)
                 trade_executed = True
-        elif action == 2: # Sell/Short
-            if self.position == 1: # Close long
+        elif action == 2:
+            if self.position == 1:
                 self.balance = self.shares_held * current_price * (1 - self.trade_fee)
                 self.position, self.shares_held, self.entry_price = 0, 0, 0
                 trade_executed = True
-            elif self.position == 0: # Open short
+            elif self.position == 0:
                 self.position, self.entry_price = -1, current_price
                 self.stop_loss_price = current_price + (current_atr * self.stop_loss_atr_multiplier)
                 self.shares_held = self.balance / current_price * (1 - self.trade_fee)
                 trade_executed = True
 
         if self.position == 1: self.portfolio_value = self.shares_held * current_price
-        elif self.position == -1:
-            self.portfolio_value = self.balance + (self.entry_price - current_price) * self.shares_held
+        elif self.position == -1: self.portfolio_value = self.balance + (self.entry_price - current_price) * self.shares_held
         else: self.portfolio_value = self.balance
 
         hourly_return = (self.portfolio_value / old_portfolio_value) - 1 if old_portfolio_value != 0 else 0
         self.returns_history.append(hourly_return)
-
         reward = 0
         if len(self.returns_history) > 21:
             relevant_returns = np.array(self.returns_history[-21:])
             mean_return = np.mean(relevant_returns)
-            
             negative_returns = relevant_returns[relevant_returns < 0]
-            if len(negative_returns) > 1:
-                downside_deviation = np.std(negative_returns)
-            else:
-                downside_deviation = 0
-            
+            downside_deviation = np.std(negative_returns) if len(negative_returns) > 1 else 0
             if downside_deviation != 0:
-                sortino_ratio = mean_return / downside_deviation * np.sqrt(252 * 7) # Annualized for hourly data
+                # Original hourly annualization factor
+                sortino_ratio = mean_return / downside_deviation * np.sqrt(252 * 7)
                 reward = np.tanh(sortino_ratio)
-            elif mean_return > 0:
-                reward = 1.0  
-            
-        if trade_executed:
-            reward -= self.trade_penalty
-
+            elif mean_return > 0: reward = 1.0
+        if trade_executed: reward -= self.trade_penalty
         current_sma_dist = self.df.iloc[self._current_step]['sma_dist']
-        if (self.position == 1 and current_sma_dist > 0) or \
-           (self.position == -1 and current_sma_dist < 0):
+        if (self.position == 1 and current_sma_dist > 0) or (self.position == -1 and current_sma_dist < 0):
             reward += self.trend_reward_bonus
-
         self._current_step += 1
         if self._current_step >= len(self.features): self.done = True
-
         obs = self.features[self._current_step] if not self.done else self.features[-1]
         return obs, reward, self.done, False, self._get_info()
 
@@ -238,13 +217,18 @@ class ReplayMemory:
     def push(self, *args): self.memory.append(Transition(*args))
     def sample(self, batch_size): return random.sample(self.memory, batch_size)
     def __len__(self): return len(self.memory)
+
 class DQN(nn.Module):
     def __init__(self, n_observations, n_actions):
         super(DQN, self).__init__()
-        self.layer1, self.layer2, self.layer3 = nn.Linear(n_observations, 128), nn.Linear(128, 128), nn.Linear(128, n_actions)
+        self.layer1 = nn.Linear(n_observations, 128)
+        self.layer2 = nn.Linear(128, 128)
+        self.layer3 = nn.Linear(128, n_actions)
     def forward(self, x):
-        x = nn.functional.relu(self.layer2(nn.functional.relu(self.layer1(x))))
+        x = nn.functional.relu(self.layer1(x))
+        x = nn.functional.relu(self.layer2(x))
         return self.layer3(x)
+
 class Agent:
     def __init__(self, env):
         self.env = env
@@ -252,29 +236,36 @@ class Agent:
         self.EPS_START, self.EPS_END, self.EPS_DECAY = 0.9, 0.05, 1000
         state, _ = self.env.reset()
         n_observations, n_actions = len(state), self.env.action_space.n
-        self.policy_net, self.target_net = DQN(n_observations, n_actions), DQN(n_observations, n_actions)
+        self.policy_net = DQN(n_observations, n_actions)
+        self.target_net = DQN(n_observations, n_actions)
         self.target_net.load_state_dict(self.policy_net.state_dict())
         self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)
-        self.memory, self.steps_done = ReplayMemory(10000), 0
+        self.memory = ReplayMemory(10000)
+        self.steps_done = 0
     def select_action(self, state):
         eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1. * self.steps_done / self.EPS_DECAY)
         self.steps_done += 1
         if random.random() > eps_threshold:
-            with torch.no_grad(): return self.policy_net(state).max(1)[1].view(1, 1)
-        else: return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)
+            with torch.no_grad():
+                return self.policy_net(state).max(1)[1].view(1, 1)
+        else:
+            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)
     def optimize_model(self):
         if len(self.memory) < self.BATCH_SIZE: return
         transitions = self.memory.sample(self.BATCH_SIZE)
         batch = Transition(*zip(*transitions))
         non_final_mask = torch.tensor(tuple(s is not None for s in batch.next_state), dtype=torch.bool)
         non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
-        state_batch, action_batch, reward_batch = torch.cat(batch.state), torch.cat(batch.action), torch.cat(batch.reward)
+        state_batch = torch.cat(batch.state)
+        action_batch = torch.cat(batch.action)
+        reward_batch = torch.cat(batch.reward)
         state_action_values = self.policy_net(state_batch).gather(1, action_batch)
         next_state_values = torch.zeros(self.BATCH_SIZE)
         with torch.no_grad():
             next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
         expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch
         loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))
-        self.optimizer.zero_grad(); loss.backward()
+        self.optimizer.zero_grad()
+        loss.backward()
         torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)
         self.optimizer.step()
\ No newline at end of file
